{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016ab603",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('my_model_pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897eeae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jetbot import Robot\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "robot = Robot()\n",
    "\n",
    "# 예측을 위해 이미지를 전처리하고 모델에 전달\n",
    "def predict(image):\n",
    "    # 이미지 전처리 (이미지는 모델에 맞게 조정되어야 함)\n",
    "    preprocessed_image = preprocess_image(image)\n",
    "    # 모델에 이미지 전달하여 예측 수행\n",
    "    predictions = model.predict(np.expand_dims(preprocessed_image, axis=0))\n",
    "    return predictions\n",
    "\n",
    "# 이미지를 전처리하는 함수 (이 부분은 모델과 입력 이미지에 따라 조정해야 함)\n",
    "def preprocess_image(image):\n",
    "    # 예: 이미지를 모델의 입력 형식에 맞게 조정\n",
    "    # 이 예에서는 이미지를 크기 224x224로 조정하고 정규화합니다.\n",
    "    image = cv2.resize(image, (224, 224))\n",
    "    image = image / 255.0  # 모델에 따라 정규화가 필요할 수 있습니다.\n",
    "    return image\n",
    "\n",
    "# JetBot을 제어하는 메인 함수\n",
    "def control_jetbot(change):\n",
    "    image = change['new']  \n",
    "    predictions = predict(image)  \n",
    "\n",
    "    red_probability = predictions[0, CATEGORIES.index('stop')]\n",
    "    orange_probability = predictions[0, CATEGORIES.index('orange')]\n",
    "    gesture_probability = predictions[0, CATEGORIES.index('dislike')]\n",
    "    green_probability = predictions[0, CATEGORIES.index('green')]\n",
    "\n",
    "    \n",
    "    if red_probability > 0.5: \n",
    "        robot.stop()  \n",
    "\n",
    "    \n",
    "    elif orange_probability > 0.5:\n",
    "        robot.backward(0.3)  \n",
    "\n",
    "    \n",
    "    elif gesture_probability > 0.5:  \n",
    "        # 고개를 옆으로 젓는 동작 수행\n",
    "        perform_gesture_action()\n",
    "        \n",
    "    elif green_probability >0.5:\n",
    "        robot.forward(0.3)\n",
    "        \n",
    "\n",
    "\n",
    "# 고개를 옆으로 젓는 동작을 수행하는 함수 (프로젝트에 맞게 수정)\n",
    "#def perform_gesture_action():\n",
    "    # 여기에 제스처 동작을 추가하세요\n",
    "    #print(\"Performing gesture action\")\n",
    "\n",
    "# 카메라 값이 변경될 때마다 JetBot을 제어하는 콜백 함수 등록\n",
    "camera.observe(control_jetbot, names='value')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6658b285",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from jetbot import Robot, Camera\n",
    "from IPython.display import display\n",
    "import ipywidgets.widgets as widgets\n",
    "from jetbot import bgr8_to_jpeg\n",
    "\n",
    "# Load the PyTorch model\n",
    "model = torch.load('sign_A/my_model.pth')\n",
    "model.eval()\n",
    "\n",
    "# Initialize the JetBot robot and camera\n",
    "robot = Robot()\n",
    "camera = Camera.instance()\n",
    "\n",
    "# Create image widget for real-time display\n",
    "image_widget = widgets.Image(format='jpeg', width=300, height=300)\n",
    "\n",
    "# Define the transform to preprocess the image\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Function to control JetBot based on color prediction\n",
    "def control_jetbot(change):\n",
    "    image = change['new']\n",
    "    \n",
    "    # Preprocess the image\n",
    "    image_tensor = transform(image).unsqueeze(0)\n",
    "    \n",
    "    # Make prediction\n",
    "    with torch.no_grad():\n",
    "        output = model(image_tensor)\n",
    "    \n",
    "    # Get the predicted color\n",
    "    _, predicted_idx = torch.max(output, 1)\n",
    "    predicted_color = DATASETS[0].categories[predicted_idx.item()]\n",
    "\n",
    "    # Update the image widget with the annotated image\n",
    "    annotated_image = annotate_image(image, predicted_color)\n",
    "    image_widget.value = bgr8_to_jpeg(annotated_image)\n",
    "\n",
    "    # Perform corresponding action based on the predicted color\n",
    "    if predicted_color == 'red':\n",
    "        robot.stop()\n",
    "    elif predicted_color == 'green':\n",
    "        robot.forward(0.3)  # Adjust speed as needed\n",
    "    elif predicted_color == 'orange':\n",
    "        robot.backward(0.3)  # Adjust speed as needed\n",
    "    elif predicted_color == 'gesture':\n",
    "        # Perform gesture action (e.g., tilt the head)\n",
    "        perform_gesture_action()\n",
    "\n",
    "# Function to perform gesture action\n",
    "def perform_gesture_action():\n",
    "    # Modify this function to perform the desired gesture action\n",
    "    print(\"Performing gesture action\")\n",
    "\n",
    "# Function to annotate the image with the predicted color\n",
    "def annotate_image(image, color):\n",
    "    # Add text to the image indicating the predicted color\n",
    "    cv2.putText(image, f'Color: {color}', (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "    return image\n",
    "\n",
    "# Register the control_jetbot function as a callback for the camera value change\n",
    "camera.observe(control_jetbot, names='value')\n",
    "\n",
    "# Display the image widget\n",
    "display(widgets.HBox([image_widget]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc9dce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet18  # 사용한 모델이 resnet18이라고 가정\n",
    "from jetbot import Robot, Camera\n",
    "from PIL import Image\n",
    "\n",
    "# 모델 생성 (여기서는 resnet18을 사용합니다. 실제로 사용한 모델에 맞게 수정 필요)\n",
    "model = resnet18(pretrained=False)  # pretrained가 False인 경우 랜덤 초기화된 모델 생성\n",
    "model.fc = torch.nn.Linear(512, len(DATASETS[0].categories))  # 출력 레이어를 예측할 클래스 수에 맞게 수정\n",
    "\n",
    "# 모델의 상태 사전 로드\n",
    "model.load_state_dict(torch.load('sign_A/my_model.pth'))\n",
    "model.eval()  # 모델을 평가 모드로 설정\n",
    "\n",
    "# Initialize the JetBot robot and camera\n",
    "robot = Robot()\n",
    "camera = Camera.instance()\n",
    "\n",
    "# 나머지 코드는 이전에 제공한 예제 코드와 동일\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af88abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.models import resnet18\n",
    "from jetbot import Robot, Camera\n",
    "from PIL import Image\n",
    "\n",
    "# 모델 생성\n",
    "model = resnet18(pretrained=False)  # 예제로 resnet18을 사용하였습니다.\n",
    "model.fc = torch.nn.Linear(512, len(DATASETS[0].categories))  # 출력 레이어를 예측할 클래스 수에 맞게 수정\n",
    "\n",
    "# 저장된 모델 파일 경로\n",
    "saved_model_path = 'my_model.pth'\n",
    "\n",
    "# 저장된 모델 가중치 불러오기\n",
    "model.load_state_dict(torch.load(saved_model_path, map_location=torch.device('cpu')))  # 저장할 때 사용한 장치에 따라 map_location 설정\n",
    "\n",
    "# 모델을 평가 모드로 설정\n",
    "model.eval()\n",
    "\n",
    "# Initialize the JetBot robot and camera\n",
    "robot = Robot()\n",
    "camera = Camera.instance()\n",
    "\n",
    "# 나머지 코드는 이전에 제공한 예제 코드와 동일\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
